\section{Auswertung einiger Testreihen}\label{sec:Testreihen}
Zur effizienten Untersuchung und Rechnung zahlreicher Beispiele wurden die in Kapitel \ref{sec:algorithmen} aufgezeigten Algorithmen in PARI/GP umgesetzt. (\cite{PARI2018})\\
Alle Beispiele wurden dabei ausschließlich mit dem in Abschnitt \ref{sec:code} gelisteten Programmcode gerechnet.

\subsection{Methodik}
Zur Testung der genannten Algorithmen wurde die folgende Methodik umgesetzt.
Alle drei Algorithmen durchliefen die Tests zwecks Vergleichbarkeit auf sehr ähnlichen Rechnern mit gleichen Bedingungen, die Laufzeiten sind somit vergleichbar.
Getestet wurde ausgehend von $n \in \N$ mit $3 \leq n \leq 10.000$.
Der Datensatz für ein gewähltes $n$ definiert sich durch die Menge der betrachteten Brüche $M_n = \left\{ \frac{x}{n} \, | \, 1\leq x < n \wedge x \nmid n\right\}$ und enthält neben dem $n$ selbst folgende Eigenschaften, die zugleich den nachgestellten Namen bekommen:
\begin{itemize}
	\item die durchschnittliche Anzahl der Summanden, \emph{avgTerms(n)}
	\item das Minimum der Anzahl der Summanden, \emph{minTerms(n)}
	\item das Maximum der Anzahl der Summanden, \emph{maxTerms(n)}
	\item das Minimum des jeweils größten Nenners, \emph{minDenom(n)}
	\item das Maximum des jeweils größten Nenners, \emph{maxDenom(n)}.
\end{itemize}
Somit entstanden 10.000 Datensätze pro Algorithmus mit jeweils 5 nutzbaren Parametern, insgesamt also 150.000 Datenpunkte, die im Folgenden ausgewertet werden sollen.

\subsection{Anfängliche Probleme und Effizienzsteigerung während der Implementierung}
In den ersten Testläufen traten einige Unannehmlichkeiten auf, da Teile des Codes in der direkten Umsetzung der formalen Beschreibung nicht immer optimal liefen.
\paragraph{Suche nach Nennern im Greedy-Algorithmus}Beispielsweise gab es im ursprünglichen Test des Greedy-Algorithmus den Fall der Zerlegung von $\frac{5}{121}$, dessen Berechnung mittels der Funktion \emph{greedy(5/121)} nach ca. 7 Stunden ohne Ergebnis abgebrochen wurde. Grund hierfür ist das Problem, den größten Stammbruch $\uf{x} \in \Q_+$ zu finden, der kleiner als ein gegebener rationaler Bruch $\frac{p}{q} \in \Q_+$ ist. In der ersten Implementierung wurde dafür in der Folge $(\uf{2}, \uf{3}, \uf{4}, \uf{5},...,\uf{x})$ gesucht, was für große Nenner offensichtlich sehr lange dauert. Nachdem eine Optimierung vorgenommen wurde, in der nun zunächst die Nenner der genannten Folge verdoppelt und dann beim Halbieren aufsummiert werden $(\uf{2}, \uf{4}, \uf{8},...,\uf{n_i}, \uf{n_i+n_j}, \uf{n_i+n_j+n_k},...,\uf{x}), \text{wobei alle } n \text{ Zweierpotenzen sind und } n_i>n_j>n_k > ... \geq 1; \, i,j,k \in \N$, steigerte sich die Effizienz des Greedy-Algorithmus enorm: Das genannte Beispiel $\frac{5}{121}$ wurde in unter einer Millisekunde korrekt berechnet, zahlreiche weitere Beispiele zeigten ähnliche Auswirkungen. Deshalb trägt diese Implementierung auch den Namen \emph{greedy\_fast(fraction)}.

\paragraph{Konstruktion der Farey-Folgen}Ein weiteres Problem stellte die Berechnung der Farey-Folgen dar. Einerseits haben solche Folgen $F_q$ selbst mit mäßig großem $q$ schon sehr viele Elemente, so ist \zB $|F_{280}| = 23.861$. Aus den Eigenschaften der Farey-Folgen kann man sehen, dass diese mit steigender Ordnung exponentiell an Größe gewinnen. Andererseits werden diese Elemente mit steigender Ordnung auch größer, was also zusätzlich Speicher belegt und Rechenzeit beansprucht. Nach den ersten paar Rechnungen wurde schnell klar, dass die vollständige Berechnung der Farey-Folgen nicht ansatzweise optimal ist und deshalb der Ansatz der Berechnung ausschließlich des relevanten Teils der Farey-Folge, wie es in Beispiel \ref{bsp:Frel} beschrieben ist, umgesetzt wurde. Dies brachte eine signifikante Effizienzsteigerung mit sich.

\subsection{Auswertung der Daten}

\subsubsection{Laufzeiten}
Die Algorithmen erreichten die nachfolgend aufgeführten Laufzeiten für die Datensätze $3 \leq n \leq 10.000$:\\
\begin{table}[H]
	\centering
	\begin{tabular}{|l | c c c c|}
		\hline
		\multicolumn{1}{|c|}{\emph{Algorithmus}} & h & min & sek & ms \\ \hline
		Binär-Algorithmus & & 33 & 22 & 336 \\ \hline
		Greedy-Algorithmus & & 40 &  53 & 499 \\ \hline
		Farey-Folgen-Algorithmus & 62 & 33 & 38 & 136 \\ \hline
	\end{tabular}
	\caption{Laufzeitvergleich der Algorithmen}
	\label{table:LaufzeitVgl}
\end{table}
Die starke Unterlegenheit des Farey-Folgen-Algorithmus ist klar erkenntlich, allerdings nicht mehr wesentlich zu verbessern. Dieses Ergebnis entstand schon mit der in Beispiel \ref{bsp:Frel} beschriebenen Methode, nur die relevante Farey-Folge zu konstruieren, da die Laufzeit mit vollständiger Konstruktion der jeweiligen Farey-Folge für jeden Datensatz $n$ noch höher war. Es lässt sich folgern, dass er damit der laufzeittechnisch schlechteste der drei Algorithmen ist.

\subsubsection{Durchschnittliche Anzahl der Terme}
Eines der wichtigsten Ergebnisse zur realistischen Abschätzung der Algorithmen gegeneinander stellt die durchschnittliche Anzahl der Terme dar, die jeweils durch die Zerlegung entstehen. Abbildung zeigt graphisch die Entwicklung. Es ist klar zu erkennen, wie sich Greedy- und Binary-Algorithmus früh bei einem relativ festen Wert mit konstanten Schwankungen einpendeln, der Ausschlag der Kurve für den Farey-Folgen-Algorithmus aber signifikant stärker ist und sich erst später bei einem wesentlich höheren Wert einpendelt. Alle Algorithmen haben aber starke Schwankungen, es gibt also keinen festen Wert, der in bestimmten Bereichen eingenommen wird.
\todo[inline]{insert graphics}

\subsubsection{Minimale Anzahl der Terme}
Bezüglich der minimalen Anzahl der produzierten Terme liefert der Greedy-Algorithmus für alle $n$ den gleichen Wert $minTerms_{greedy}(n) = 2$, Der Farey-Folgen-Algorithmus liefert ständig zwischen $2$ und $3$ alternierende Werte, wobei $n=6$ eine Ausnahme liefert mit $minTerms_{farey}(6) = 5$, da aufgrund der vielen Kürzungen im Datensatz für $n=6$ nur $\frac{5}{6}$ betrachtet wird, welcher in der Farey-Zerlegung 5 Terme benötigt. Beim Binär-Algorithmus erreichen die Meisten Tests ein Minimum von 2 Termen pro Datensatz, in ca. $0,7\%$ Fälle liegt das Minimum jedoch bei 3 Termen. Ein besonderer Zusammenhang zwischen solchen Datensätzen wurde nicht gefunden.
\todo[inline]{insert graphics}

\subsubsection{Maximale Anzahl der Terme}
 Der Greedy-Algorithmus pendelt sich ab $n>3000$ bei Werten von $7 \leq maxTerms_{greedy}(n) \leq 16$ ein, während der Binäralgorithmus sich ab $n>2000$ mit Werten von $15 \leq maxDenom_{binary}(n) \leq 22$ einpendelt, wobei einzelne Abweichungen nach oben und unten vernachlässigt wurden. Für den Farey-Folgen-Algorithmus hingegen gilt ein lineares Wachstum:
$$maxTerms_{farey}(n) = n + 1, \, \forall n$$
für \todo{finde Erklärung}alle getesteten Datensätze. 
\todo[inline]{insert graphics}

\subsubsection{Minimum der größten Nenner}
In dieser Hinsicht zeichnet sich ein besonders interessantes Bild ab. Alle Algorithmen zeigen klares lineares Wachstum bezüglich $minDenom(n)$, allerdings in sehr unterschiedlichen Ausprägungen. Der Binäralgorithmus bewegt sich zwischen den Werten der anderen Algorithmen und ist, bis auf wenige Ausnahmen, durch eine einzige Gerade erkenntlich. Der Greedy-Algorithmus, dessen Werte dieses Kriteriums unterhalb derer des Binäralgorithmus liegen, zeigt hingegen mehrere Geraden auf, also entwickeln sich bestimmte Folgen innerhalb der Testreihe dieses Algorithmus gleich, die Folgen sind aber in ihrer Entwicklung voneinander unterscheidbar. Gleiches gilt für den mit größeren Werten ergebenden Farey-Folgen-Algorithmus, für den sich 5 geraden ergeben, allerdings wird hier die Anzahl der Datenpunkte, die jeweils eine Gerade ergeben, mit wachsendem Anstieg selbiger, immer weniger. \emph{Es ist zu vermuten, dass dies von gewissen Modulae abhängt, weitere Erkenntnisse fehlen noch}
\todo[inline]{insert graphics}
\subsubsection{Maximum der größten Nenner}
Dieses Kriterium ist jenes, in welchem sich die Algorithmen wohl am stärksten voneinander unterscheiden. Während sich die Werte von Binär- und Farey-Folgen-Algorithmus unter $10^{19}$ halten, streuen sich die Werte des Greedy-Algorithmus von $maxDenom_{greed}(3) = 2$ bis zu einem Maximum von $maxDenom_{greedy}(4967) \approx 7,3378 \times 10^{225.516}$ ohne jegliche erkennbare Regelmäßigkeit. \emph{binary \& farey relativ regelmäßig (Vgl. quadratisch o.ä.?)}
\todo[inline]{insert graphics}
\subsubsection{Zusammenhang innerhalb des Binary-Algorithmus}
Im Binary-Algorithmus konnte eine unerwartete Korrelation zwischen $n$ und $minDenom(n)$ gemacht werden:
\begin{equation*}
	minDenom(n) = 
	\begin{cases}
		n & \text{falls n Zweierpotenz ist} \\
		2n & \text{sonst.}
	\end{cases}
\end{equation*}

\subsection{Zusammenfassung}
\begin{itemize}
	\item greedy zwar der beste bzgl. \#Terms, aber mit maxDenom fast unbrauchbar $\rightarrow$ sehr unvorhersehbar/ unkontrollierbar
	\item Farey liefert zwar brauchbare Größenordnung für die Länge der Nenner, aber dafür extrem viele Terme
	\item Binär ist wohl der am stabilsten laufende und brauchbarste, zudem der schnellste der getesteten Verfahren.
\end{itemize}